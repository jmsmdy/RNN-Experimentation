{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PRE-PROCESSING STEP 1\n",
    "# WARNING: This cell will take a while to run, and uses a significant amount of memory\n",
    "\n",
    "from nltk.corpus.reader.bnc import BNCCorpusReader\n",
    "import os\n",
    "import json\n",
    "\n",
    "# load all sentences from corpus\n",
    "BNC_root_dir = os.path.join('DATA', 'ota_20.500.12024_2554', 'download', 'Texts')\n",
    "bcr = BNCCorpusReader(BNC_root_dir, fileids=r'[A-K]/\\w*/\\w*\\.xml')\n",
    "all_sents = bcr.sents()\n",
    "\n",
    "# filter for sentences ending in periods (to filter out questions, titles, etc.)\n",
    "sents = list(filter(lambda s: len(s) > 0 and s[-1] == '.', all_sents))\n",
    "\n",
    "# save list of sentences to JSON file\n",
    "with open(os.path.join('DATA', 'sents.json'), 'w') as f:\n",
    "    json.dump(sents, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PRE-PROCESSING STEP 2\n",
    "# WARNING: This cell will take a while to run, and uses a significant amount of memory\n",
    "\n",
    "import os\n",
    "import json\n",
    "\n",
    "with open(os.path.join('DATA', 'sents.json'), 'r') as f:\n",
    "    sents = json.load(f)\n",
    "\n",
    "all_tokens_list = itertools.chain(sents)\n",
    "all_tokens_set = set(all_tokens_list)\n",
    "token_freqs = {token : 0 for token in all_tokens_set}\n",
    "\n",
    "for token in all_tokens_list:\n",
    "    token_freqs[token] += 1\n",
    "    \n",
    "top_10000 = sorted(token_freqs.items(), key=lambda x: x[1], reverse=True)[:10000]\n",
    "top_10000_dict = {x[0] : idx for idx, x in enumerate(top_10000)}\n",
    "\n",
    "numberize = {}\n",
    "for token in all_tokens_set:\n",
    "    if token in top_10000:\n",
    "        numberize[token] = top_10000_dict[token]\n",
    "    else:\n",
    "        numberize[token] = 10000\n",
    "\n",
    "with open(os.path.join('DATA', 'numberize.json'), 'w') as f:\n",
    "    json.dump(numberize, f)\n",
    "        \n",
    "reverse_numberize = {10000 : '<UNK>'}\n",
    "for token, idx in top_10000_dict.items():\n",
    "    reverse_numberize[idx] = token\n",
    "    \n",
    "with open(os.path.join('DATA', 'reverse_numberize.json'), 'w') as f:\n",
    "    json.dump(reverse_numberize, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
