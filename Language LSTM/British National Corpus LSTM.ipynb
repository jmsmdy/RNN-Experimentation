{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing (run only once)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PRE-PROCESSING STEP 1\n",
    "# WARNING: This cell will take a while to run, and uses a significant amount of memory\n",
    "\n",
    "from nltk.corpus.reader.bnc import BNCCorpusReader\n",
    "import os\n",
    "import json\n",
    "\n",
    "# load all sentences from corpus\n",
    "BNC_root_dir = os.path.join('DATA', 'ota_20.500.12024_2554', 'download', 'Texts')\n",
    "bcr = BNCCorpusReader(BNC_root_dir, fileids=r'[A-K]/\\w*/\\w*\\.xml')\n",
    "all_sents = bcr.sents()\n",
    "\n",
    "# filter for sentences ending in periods (to filter out questions, titles, etc.)\n",
    "sents = list(filter(lambda s: len(s) > 0 and s[-1] == '.', all_sents))\n",
    "\n",
    "# save list of sentences to JSON file\n",
    "with open(os.path.join('DATA', 'sents.json'), 'w') as f:\n",
    "    json.dump(sents, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may reset the kernel to free memory between preprocessing steps 2 and 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PRE-PROCESSING STEP 2\n",
    "# WARNING: This cell will take a while to run, and uses a significant amount of memory\n",
    "\n",
    "import os\n",
    "import json\n",
    "import itertools\n",
    "\n",
    "with open(os.path.join('DATA', 'sents.json'), 'r') as f:\n",
    "    sents = json.load(f)\n",
    "\n",
    "all_tokens_list = list(itertools.chain.from_iterable(sents))\n",
    "all_tokens_set = set(all_tokens_list)\n",
    "token_freqs = {token : 0 for token in all_tokens_set}\n",
    "\n",
    "for token in all_tokens_list:\n",
    "    token_freqs[token] += 1\n",
    "    \n",
    "top_10000 = sorted(token_freqs.items(), key=lambda x: x[1], reverse=True)[:10000]\n",
    "top_10000_dict = {x[0] : idx for idx, x in enumerate(top_10000)}\n",
    "\n",
    "numberize = {}\n",
    "for token in all_tokens_set:\n",
    "    if token in top_10000_dict.keys():\n",
    "        numberize[token] = top_10000_dict[token]\n",
    "    else:\n",
    "        numberize[token] = 10000\n",
    "\n",
    "with open(os.path.join('DATA', 'numberize.json'), 'w') as f:\n",
    "    json.dump(numberize, f)\n",
    "        \n",
    "reverse_numberize = {10000 : '<UNK>', 10001 : '<EOS>'}\n",
    "for token, idx in top_10000_dict.items():\n",
    "    reverse_numberize[idx] = token\n",
    "    \n",
    "with open(os.path.join('DATA', 'reverse_numberize.json'), 'w') as f:\n",
    "    json.dump(reverse_numberize, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may reset the kernel to free memory between pre-processing steps 2 and 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PRE-PROCESSING STEP 3\n",
    "# WARNING: This cell will take a while to run, and uses a significant amount of memory\n",
    "\n",
    "import os\n",
    "import json\n",
    "\n",
    "with open(os.path.join('DATA', 'sents.json'), 'r') as f:\n",
    "    sents = json.load(f)\n",
    "\n",
    "with open(os.path.join('DATA', 'numberize.json'), 'r') as f:\n",
    "    numberize = json.load(f)\n",
    "\n",
    "# We filter out sentence 30 tokens or longer, and fill the end with <EOS> tokens.\n",
    "numberized_sents = [[numberize[token] for token in sent] + [10001]*(30 - len(sent)) for sent in sents if len(sent) < 30]\n",
    "\n",
    "with open(os.path.join('DATA', 'numberized_sents.json'), 'w') as f:\n",
    "    json.dump(numberized_sents, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may reset the kernel to free memory between pre-processing steps 3 and 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PRE-PROCESSING STEP 4\n",
    "# WARNING: This cell will take a while to run, and uses a significant amount of memory\n",
    "\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "with open(os.path.join('DATA', 'numberized_sents.json'), 'r') as f:\n",
    "    numberized_sents = json.load(f)\n",
    "\n",
    "if not os.path.exists(os.path.join('DATA', 'MINIBATCHES')):\n",
    "    os.makedirs(os.path.join('DATA', 'MINIBATCHES'))\n",
    "minibatch_size = 500\n",
    "num_full_minibatches = len(numberized_sents) // minibatch_size\n",
    "\n",
    "for i in range(num_full_minibatches):\n",
    "    minibatch = [\n",
    "        [[1 if i == token_num else 0 for i in range(10002)] for token_num in sent]\n",
    "        for sent in numberized_sents[i*minibatch_size:(i+1)*minibatch_size]\n",
    "    ]\n",
    "    minibatch_tensor = tf.constant(minibatch, dtype=tf.int32)\n",
    "    sparse = tf.sparse.from_dense(minibatch_tensor)\n",
    "    np.save(os.path.join('DATA', 'MINIBATCHES', f'{i}_indices.npy'), sparse.indices.numpy())\n",
    "    np.save(os.path.join('DATA', 'MINIBATCHES', f'{i}_values.npy'), sparse.values.numpy())\n",
    "    np.save(os.path.join('DATA', 'MINIBATCHES', f'{i}_shape.npy'), np.array(sparse.shape))\n",
    "    \n",
    "if len(numberized_sents) % minibatch_size != 0:\n",
    "    minibatch = [\n",
    "        [[1 if i == token_num else 0 for i in range(10002)] for token_num in sent]\n",
    "        for sent in numberized_sents[(num_full_minibatches)*minibatch_size:len(numberized_sents)]\n",
    "    ]\n",
    "    minibatch_tensor = tf.constant(minibatch, dtype=tf.int32)\n",
    "    sparse = tf.sparse.from_dense(minibatch_tensor)\n",
    "    np.save(os.path.join('DATA', 'MINIBATCHES', f'{num_full_minibatches}_indices.npy'), sparse.indices.numpy())\n",
    "    np.save(os.path.join('DATA', 'MINIBATCHES', f'{num_full_minibatches}_values.npy'), sparse.values.numpy())\n",
    "    np.save(os.path.join('DATA', 'MINIBATCHES', f'{num_full_minibatches}_shape.npy'), np.array(sparse.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
