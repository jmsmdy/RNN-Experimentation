{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing (run only once)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PRE-PROCESSING STEP 1\n",
    "# WARNING: This cell will take a while to run, and uses a significant amount of memory\n",
    "\n",
    "from nltk.corpus.reader.bnc import BNCCorpusReader\n",
    "import os\n",
    "import json\n",
    "\n",
    "# load all sentences from corpus\n",
    "BNC_root_dir = os.path.join('DATA', 'ota_20.500.12024_2554', 'download', 'Texts')\n",
    "bcr = BNCCorpusReader(BNC_root_dir, fileids=r'[A-K]/\\w*/\\w*\\.xml')\n",
    "all_sents = bcr.sents()\n",
    "\n",
    "# filter for sentences ending in periods (to filter out questions, titles, etc.)\n",
    "sents = list(filter(lambda s: len(s) > 0 and s[-1] == '.', all_sents))\n",
    "\n",
    "# save list of sentences to JSON file\n",
    "with open(os.path.join('DATA', 'sents.json'), 'w') as f:\n",
    "    json.dump(sents, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may reset the kernel to free memory between preprocessing steps 2 and 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PRE-PROCESSING STEP 2\n",
    "# WARNING: This cell will take a while to run, and uses a significant amount of memory\n",
    "\n",
    "import os\n",
    "import json\n",
    "import itertools\n",
    "\n",
    "with open(os.path.join('DATA', 'sents.json'), 'r') as f:\n",
    "    sents = json.load(f)\n",
    "\n",
    "all_tokens_list = list(itertools.chain.from_iterable(sents))\n",
    "all_tokens_set = set(all_tokens_list)\n",
    "token_freqs = {token : 0 for token in all_tokens_set}\n",
    "\n",
    "for token in all_tokens_list:\n",
    "    token_freqs[token] += 1\n",
    "    \n",
    "top_10000 = sorted(token_freqs.items(), key=lambda x: x[1], reverse=True)[:10000]\n",
    "top_10000_dict = {x[0] : idx for idx, x in enumerate(top_10000)}\n",
    "\n",
    "numberize = {}\n",
    "for token in all_tokens_set:\n",
    "    if token in top_10000_dict.keys():\n",
    "        numberize[token] = top_10000_dict[token]\n",
    "    else:\n",
    "        numberize[token] = 10000\n",
    "\n",
    "with open(os.path.join('DATA', 'numberize.json'), 'w') as f:\n",
    "    json.dump(numberize, f)\n",
    "        \n",
    "reverse_numberize = {10000 : '<UNK>', 10001 : '<EOS>'}\n",
    "for token, idx in top_10000_dict.items():\n",
    "    reverse_numberize[idx] = token\n",
    "    \n",
    "with open(os.path.join('DATA', 'reverse_numberize.json'), 'w') as f:\n",
    "    json.dump(reverse_numberize, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may reset the kernel to free memory between pre-processing steps 2 and 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PRE-PROCESSING STEP 3\n",
    "# WARNING: This cell will take a while to run, and uses a significant amount of memory\n",
    "\n",
    "import os\n",
    "import json\n",
    "\n",
    "with open(os.path.join('DATA', 'sents.json'), 'r') as f:\n",
    "    sents = json.load(f)\n",
    "\n",
    "with open(os.path.join('DATA', 'numberize.json'), 'r') as f:\n",
    "    numberize = json.load(f)\n",
    "\n",
    "# We filter out sentence 30 tokens or longer\n",
    "numberized_sents = [[numberize[token] for token in sent] + [10001] for sent in sents if len(sent) < 30]\n",
    "\n",
    "with open(os.path.join('DATA', 'numberized_sents.json'), 'w') as f:\n",
    "    json.dump(numberized_sents, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may reset the kernel to free memory between pre-processing steps 3 and 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "with open(os.path.join('DATA', 'numberized_sents.json'), 'r') as f:\n",
    "    numberized_sents = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3403257"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(numberized_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PRE-PROCESSING STEP 4\n",
    "# WARNING: This cell will take a while to run, and uses a significant amount of memory\n",
    "\n",
    "import os\n",
    "import json\n",
    "\n",
    "with open(os.path.join('DATA', 'numberized_sents.json'), 'r') as f:\n",
    "    numberized_sents = json.load(f)\n",
    "    \n",
    "vectorized_sents = [[[1 if i == word_num else 0 for i in range(10002)] for word_num in sent] for sent in numberized_sents]\n",
    "\n",
    "with open(os.path.join('DATA', 'vectorized_sents.json'), 'w') as f:\n",
    "    json.dump(vectorized_sents, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
